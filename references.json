[
  {
    "id": "1Cb94HrPD",
    "URL": "https://arxiv.org/abs/1611.03530",
    "number": "1611.03530",
    "title": "Understanding deep learning requires rethinking generalization",
    "issued": {
      "date-parts": [
        [
          2017,
          2,
          28
        ]
      ]
    },
    "author": [
      {
        "given": "Chiyuan",
        "family": "Zhang"
      },
      {
        "given": "Samy",
        "family": "Bengio"
      },
      {
        "given": "Moritz",
        "family": "Hardt"
      },
      {
        "given": "Benjamin",
        "family": "Recht"
      },
      {
        "given": "Oriol",
        "family": "Vinyals"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.\n  Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.\n  We interpret our experimental findings by comparison with traditional models. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: arxiv:1611.03530"
  },
  {
    "publisher": "Elsevier BV",
    "DOI": "10.1016/s0960-9776(09)70290-5",
    "type": "article-journal",
    "page": "S141-S145",
    "source": "Crossref",
    "title": "Prediction of adjuvant chemotherapy benefit in endocrine responsive, early breast cancer using multigene assays",
    "volume": "18",
    "author": [
      {
        "given": "Kathy S.",
        "family": "Albain"
      },
      {
        "given": "Soonmyung",
        "family": "Paik"
      },
      {
        "given": "Laura",
        "family": "van't Veer"
      }
    ],
    "container-title": "The Breast",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2009,
          10
        ]
      ]
    },
    "URL": "https://doi.org/bp4rtw",
    "container-title-short": "The Breast",
    "PMID": "19914534",
    "id": "OW5P00JC",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/s0960-9776(09)70290-5"
  },
  {
    "publisher": "Proceedings of the National Academy of Sciences",
    "issue": "48",
    "abstract": "<jats:p>The phenomenon of benign overfitting is one of the key mysteries uncovered by deep learning methodology: deep neural networks seem to predict well, even with a perfect fit to noisy training data. Motivated by this phenomenon, we consider when a perfect fit to training data in linear regression is compatible with accurate prediction. We give a characterization of linear regression problems for which the minimum norm interpolating prediction rule has near-optimal prediction accuracy. The characterization is in terms of two notions of the effective rank of the data covariance. It shows that overparameterization is essential for benign overfitting in this setting: the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size. By studying examples of data covariance properties that this characterization shows are required for benign overfitting, we find an important role for finite-dimensional data: the accuracy of the minimum norm interpolating prediction rule approaches the best possible accuracy for a much narrower range of properties of the data distribution when the data lie in an infinite-dimensional space vs. when the data lie in a finite-dimensional space with dimension that grows faster than the sample size.</jats:p>",
    "DOI": "10.1073/pnas.1907378117",
    "type": "article-journal",
    "page": "30063-30070",
    "source": "Crossref",
    "title": "Benign overfitting in linear regression",
    "volume": "117",
    "author": [
      {
        "given": "Peter L.",
        "family": "Bartlett"
      },
      {
        "given": "Philip M.",
        "family": "Long"
      },
      {
        "given": "Gábor",
        "family": "Lugosi"
      },
      {
        "given": "Alexander",
        "family": "Tsigler"
      }
    ],
    "container-title": "Proceedings of the National Academy of Sciences",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2020,
          4,
          24
        ]
      ]
    },
    "URL": "https://doi.org/gjgsxq",
    "container-title-short": "Proc. Natl. Acad. Sci. U.S.A.",
    "PMCID": "PMC7720150",
    "PMID": "32332161",
    "id": "jhZXoLtp",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1073/pnas.1907378117"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "11",
    "abstract": "<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>Assigning every human gene to specific functions, diseases and traits is a grand challenge in modern genetics. Key to addressing this challenge are computational methods, such as supervised learning and label propagation, that can leverage molecular interaction networks to predict gene attributes. In spite of being a popular machine-learning technique across fields, supervised learning has been applied only in a few network-based studies for predicting pathway-, phenotype- or disease-associated genes. It is unknown how supervised learning broadly performs across different networks and diverse gene classification tasks, and how it compares to label propagation, the widely benchmarked canonical approach for this problem.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>In this study, we present a comprehensive benchmarking of supervised learning for network-based gene classification, evaluating this approach and a classic label propagation technique on hundreds of diverse prediction tasks and multiple networks using stringent evaluation schemes. We demonstrate that supervised learning on a gene’s full network connectivity outperforms label propagaton and achieves high prediction accuracy by efficiently capturing local network properties, rivaling label propagation’s appeal for naturally using network topology. We further show that supervised learning on the full network is also superior to learning on node embeddings (derived using node2vec), an increasingly popular approach for concisely representing network connectivity. These results show that supervised learning is an accurate approach for prioritizing genes associated with diverse functions, diseases and traits and should be considered a staple of network-based gene classification workflows.</jats:p></jats:sec><jats:sec><jats:title>Availability and implementation</jats:title><jats:p>The datasets and the code used to reproduce the results and add new gene classification methods have been made freely available.</jats:p></jats:sec><jats:sec><jats:title>Contact</jats:title><jats:p>arjun@msu.edu</jats:p></jats:sec><jats:sec><jats:title>Supplementary information</jats:title><jats:p>Supplementary data are available at Bioinformatics online.</jats:p></jats:sec>",
    "DOI": "10.1093/bioinformatics/btaa150",
    "type": "article-journal",
    "page": "3457-3465",
    "source": "Crossref",
    "title": "Supervised learning is an accurate method for network-based gene classification",
    "volume": "36",
    "author": [
      {
        "given": "Renming",
        "family": "Liu"
      },
      {
        "given": "Christopher A",
        "family": "Mancuso"
      },
      {
        "given": "Anna",
        "family": "Yannakopoulos"
      },
      {
        "given": "Kayla A",
        "family": "Johnson"
      },
      {
        "given": "Arjun",
        "family": "Krishnan"
      }
    ],
    "container-title": "Bioinformatics",
    "language": "en",
    "editor": [
      {
        "given": "Zhiyong",
        "family": "Lu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          4,
          14
        ]
      ]
    },
    "URL": "https://doi.org/gmvnfc",
    "PMCID": "PMC7267831",
    "PMID": "32129827",
    "id": "bOvwnoRq",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/bioinformatics/btaa150"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Achieving precision oncology requires accurate identification of targetable cancer vulnerabilities in patients. Generally, genomic features are regarded as the state-of-the-art method for stratifying patients for targeted therapies. In this work, we conduct the first rigorous comparison of DNA- and expression-based predictive models for viability across five datasets encompassing chemical and genetic perturbations. We find that expression consistently outperforms DNA for predicting vulnerabilities, including many currently stratified by canonical DNA markers. Contrary to their perception in the literature, the most accurate expression-based models depend on few features and are amenable to biological interpretation. This work points to the importance of exploring more comprehensive expression profiling in clinical settings.</jats:p>",
    "DOI": "10.1101/2020.02.21.959627",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Gene expression has more power for predicting <i>in vitro</i> cancer cell vulnerabilities than genomics",
    "author": [
      {
        "given": "Joshua M.",
        "family": "Dempster"
      },
      {
        "given": "John M.",
        "family": "Krill-Burger"
      },
      {
        "given": "James M.",
        "family": "McFarland"
      },
      {
        "given": "Allison",
        "family": "Warren"
      },
      {
        "given": "Jesse S.",
        "family": "Boehm"
      },
      {
        "given": "Francisca",
        "family": "Vazquez"
      },
      {
        "given": "William C.",
        "family": "Hahn"
      },
      {
        "given": "Todd R.",
        "family": "Golub"
      },
      {
        "given": "Aviad",
        "family": "Tsherniak"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          2,
          24
        ]
      ]
    },
    "URL": "https://doi.org/ghczbr",
    "id": "1CcaMqQZg",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2020.02.21.959627"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Preclinical models like cancer cell lines and patient-derived xenografts (PDXs) are vital for studying disease mechanisms and evaluating treatment options. It is essential that they accurately recapitulate the disease state of interest to generate results that will translate in the clinic. Prior studies have demonstrated that preclinical models do not recapitulate all biological aspects of human tissues, particularly with respect to the tissue of origin gene expression signatures. Therefore, it is critical to assess how well preclinical model gene expression profiles correlate with human cancer tissues to inform preclinical model selection and data analysis decisions. Here we evaluated how well preclinical models recapitulate human cancer and non-diseased tissue gene expression patterns with respect to the most variable genes, tumor purity, and tissue specificity by using publicly available gene expression profiles across multiple sources. We found that using the full gene set improves correlations between preclinical model and tissue global gene expression profiles, confirmed that GBM PDX global gene expression correlation to GBM tumor global gene expression outperforms GBM cell line to GBM tumor global gene expression correlations, and demonstrated that preclinical models in our study often failed to reproduce tissue-specific expression. While including additional genes for global gene expression comparison between cell lines and tissues decreases the overall correlation, it improves the relative rank between a cell line and its tissue of origin compared to other tissues. Our findings underscore the importance of using the full gene expression set measured when comparing preclinical models and tissues and confirm that tissue-specific patterns are better preserved in GBM PDX models than in GBM cell lines. Future studies can build on these findings to determine the specific pathways and gene sets recapitulated by particular preclinical models to facilitate model selection for a given study design or goal.</jats:p>",
    "DOI": "10.1101/2023.04.11.536431",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Evaluating cancer cell line and patient-derived xenograft recapitulation of tumor and non-diseased tissue gene expression profiles",
    "author": [
      {
        "given": "Avery S.",
        "family": "Williams"
      },
      {
        "given": "Elizabeth J.",
        "family": "Wilk"
      },
      {
        "given": "Jennifer L.",
        "family": "Fisher"
      },
      {
        "given": "Brittany N.",
        "family": "Lasseigne"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          4,
          13
        ]
      ]
    },
    "URL": "https://doi.org/gr6jr4",
    "PMCID": "PMC10120639",
    "PMID": "37090499",
    "id": "lhYg2Yc3",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2023.04.11.536431"
  },
  {
    "publisher": "American Association for the Advancement of Science (AAAS)",
    "issue": "6127",
    "DOI": "10.1126/science.1235122",
    "type": "article-journal",
    "page": "1546-1558",
    "source": "Crossref",
    "title": "Cancer Genome Landscapes",
    "volume": "339",
    "author": [
      {
        "given": "B.",
        "family": "Vogelstein"
      },
      {
        "given": "N.",
        "family": "Papadopoulos"
      },
      {
        "given": "V. E.",
        "family": "Velculescu"
      },
      {
        "given": "S.",
        "family": "Zhou"
      },
      {
        "given": "L. A.",
        "family": "Diaz"
      },
      {
        "given": "K. W.",
        "family": "Kinzler"
      }
    ],
    "container-title": "Science",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2013,
          3,
          28
        ]
      ]
    },
    "URL": "https://doi.org/6rg",
    "container-title-short": "Science",
    "PMCID": "PMC3749880",
    "PMID": "23539594",
    "id": "809OyWlC",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1126/science.1235122"
  },
  {
    "publisher": "Association for Computing Machinery (ACM)",
    "issue": "3",
    "abstract": "<jats:p>Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training.</jats:p>\n          <jats:p>Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.</jats:p>\n          <jats:p>We interpret our experimental findings by comparison with traditional models.</jats:p>\n          <jats:p>We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.</jats:p>",
    "DOI": "10.1145/3446776",
    "type": "article-journal",
    "page": "107-115",
    "source": "Crossref",
    "title": "Understanding deep learning (still) requires rethinking generalization",
    "volume": "64",
    "author": [
      {
        "given": "Chiyuan",
        "family": "Zhang"
      },
      {
        "given": "Samy",
        "family": "Bengio"
      },
      {
        "given": "Moritz",
        "family": "Hardt"
      },
      {
        "given": "Benjamin",
        "family": "Recht"
      },
      {
        "given": "Oriol",
        "family": "Vinyals"
      }
    ],
    "container-title": "Communications of the ACM",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2021,
          2,
          22
        ]
      ]
    },
    "URL": "https://doi.org/gh57fd",
    "container-title-short": "Commun. ACM",
    "id": "1HdGWY10w",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/3446776"
  },
  {
    "publisher": "American Society of Clinical Oncology (ASCO)",
    "issue": "8",
    "abstract": "<jats:sec><jats:title>Purpose</jats:title><jats:p> To improve on current standards for breast cancer prognosis and prediction of chemotherapy benefit by developing a risk model that incorporates the gene expression–based “intrinsic” subtypes luminal A, luminal B, HER2-enriched, and basal-like. </jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p> A 50-gene subtype predictor was developed using microarray and quantitative reverse transcriptase polymerase chain reaction data from 189 prototype samples. Test sets from 761 patients (no systemic therapy) were evaluated for prognosis, and 133 patients were evaluated for prediction of pathologic complete response (pCR) to a taxane and anthracycline regimen. </jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p> The intrinsic subtypes as discrete entities showed prognostic significance (P = 2.26E-12) and remained significant in multivariable analyses that incorporated standard parameters (estrogen receptor status, histologic grade, tumor size, and node status). A prognostic model for node-negative breast cancer was built using intrinsic subtype and clinical information. The C-index estimate for the combined model (subtype and tumor size) was a significant improvement on either the clinicopathologic model or subtype model alone. The intrinsic subtype model predicted neoadjuvant chemotherapy efficacy with a negative predictive value for pCR of 97%. </jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p> Diagnosis by intrinsic subtype adds significant prognostic and predictive information to standard parameters for patients with breast cancer. The prognostic properties of the continuous risk score will be of value for the management of node-negative breast cancers. The subtypes and risk score can also be used to assess the likelihood of efficacy from neoadjuvant chemotherapy. </jats:p></jats:sec>",
    "DOI": "10.1200/jco.2008.18.1370",
    "type": "article-journal",
    "page": "1160-1167",
    "source": "Crossref",
    "title": "Supervised Risk Predictor of Breast Cancer Based on Intrinsic Subtypes",
    "volume": "27",
    "author": [
      {
        "given": "Joel S.",
        "family": "Parker"
      },
      {
        "given": "Michael",
        "family": "Mullins"
      },
      {
        "given": "Maggie C.U.",
        "family": "Cheang"
      },
      {
        "given": "Samuel",
        "family": "Leung"
      },
      {
        "given": "David",
        "family": "Voduc"
      },
      {
        "given": "Tammi",
        "family": "Vickery"
      },
      {
        "given": "Sherri",
        "family": "Davies"
      },
      {
        "given": "Christiane",
        "family": "Fauron"
      },
      {
        "given": "Xiaping",
        "family": "He"
      },
      {
        "given": "Zhiyuan",
        "family": "Hu"
      },
      {
        "given": "John F.",
        "family": "Quackenbush"
      },
      {
        "given": "Inge J.",
        "family": "Stijleman"
      },
      {
        "given": "Juan",
        "family": "Palazzo"
      },
      {
        "given": "J.S.",
        "family": "Marron"
      },
      {
        "given": "Andrew B.",
        "family": "Nobel"
      },
      {
        "given": "Elaine",
        "family": "Mardis"
      },
      {
        "given": "Torsten O.",
        "family": "Nielsen"
      },
      {
        "given": "Matthew J.",
        "family": "Ellis"
      },
      {
        "given": "Charles M.",
        "family": "Perou"
      },
      {
        "given": "Philip S.",
        "family": "Bernard"
      }
    ],
    "container-title": "Journal of Clinical Oncology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2009,
          3,
          10
        ]
      ]
    },
    "URL": "https://doi.org/c2688w",
    "container-title-short": "JCO",
    "PMCID": "PMC2667820",
    "PMID": "19204204",
    "id": "lnK82Ey6",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1200/jco.2008.18.1370"
  },
  {
    "publisher": "Public Library of Science (PLoS)",
    "issue": "3",
    "abstract": "<jats:p>By classifying patients into subgroups, clinicians can provide more effective care than using a uniform approach for all patients. Such subgroups might include patients with a particular disease subtype, patients with a good (or poor) prognosis, or patients most (or least) likely to respond to a particular therapy. Transcriptomic measurements reflect the downstream effects of genomic and epigenomic variations. However, high-throughput technologies generate thousands of measurements per patient, and complex dependencies exist among genes, so it may be infeasible to classify patients using traditional statistical models. Machine-learning classification algorithms can help with this problem. However, hundreds of classification algorithms exist—and most support diverse hyperparameters—so it is difficult for researchers to know which are optimal for gene-expression biomarkers. We performed a benchmark comparison, applying 52 classification algorithms to 50 gene-expression datasets (143 class variables). We evaluated algorithms that represent diverse machine-learning methodologies and have been implemented in general-purpose, open-source, machine-learning libraries. When available, we combined clinical predictors with gene-expression data. Additionally, we evaluated the effects of performing hyperparameter optimization and feature selection using nested cross validation. Kernel- and ensemble-based algorithms consistently outperformed other types of classification algorithms; however, even the top-performing algorithms performed poorly in some cases. Hyperparameter optimization and feature selection typically improved predictive performance, and univariate feature-selection algorithms typically outperformed more sophisticated methods. Together, our findings illustrate that algorithm performance varies considerably when other factors are held constant and thus that algorithm selection is a critical step in biomarker studies.</jats:p>",
    "DOI": "10.1371/journal.pcbi.1009926",
    "type": "article-journal",
    "page": "e1009926",
    "source": "Crossref",
    "title": "The ability to classify patients based on gene-expression data varies by algorithm and performance metric",
    "volume": "18",
    "author": [
      {
        "given": "Stephen R.",
        "family": "Piccolo"
      },
      {
        "given": "Avery",
        "family": "Mecham"
      },
      {
        "given": "Nathan P.",
        "family": "Golightly"
      },
      {
        "given": "Jérémie L.",
        "family": "Johnson"
      },
      {
        "given": "Dustin B.",
        "family": "Miller"
      }
    ],
    "container-title": "PLOS Computational Biology",
    "language": "en",
    "editor": [
      {
        "given": "Xing",
        "family": "Chen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022,
          3,
          11
        ]
      ]
    },
    "URL": "https://doi.org/gr43qd",
    "container-title-short": "PLoS Comput Biol",
    "PMCID": "PMC8942277",
    "PMID": "35275931",
    "id": "1CdDn310M",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1371/journal.pcbi.1009926"
  },
  {
    "publisher": "Foundation for Open Access Statistic",
    "issue": "1",
    "DOI": "10.18637/jss.v033.i01",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Regularization Paths for Generalized Linear Models via Coordinate Descent",
    "volume": "33",
    "author": [
      {
        "given": "Jerome",
        "family": "Friedman"
      },
      {
        "given": "Trevor",
        "family": "Hastie"
      },
      {
        "given": "Robert",
        "family": "Tibshirani"
      }
    ],
    "container-title": "Journal of Statistical Software",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "URL": "https://doi.org/bb3d",
    "container-title-short": "J. Stat. Soft.",
    "id": "10aVUDFmA",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.18637/jss.v033.i01"
  },
  {
    "id": "iAznO95g",
    "type": "paper-conference",
    "publisher": "Association for the Advancement of Artificial Intelligence (AAAI)",
    "container-title": "AAAI 2006",
    "URL": "https://ai.stanford.edu/~pabbeel//pubs/LeeLeeAbbeelNg_el1rlr_AAAI2006.pdf",
    "title": "Efficient L1 Regularized Logistic Regression",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "author": [
      {
        "given": "Su-In",
        "family": "Lee"
      },
      {
        "given": "Honglak",
        "family": "Lee"
      },
      {
        "given": "Pieter",
        "family": "Abbeel"
      },
      {
        "given": "Andrew Y.",
        "family": "Ng"
      }
    ],
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: l1-logistic-regression"
  },
  {
    "id": "x24AA6cY",
    "type": "webpage",
    "URL": "https://wiki.eecs.yorku.ca/course_archive/2012-13/F/6328/_media/bottou-onlinelearning-98.pdf",
    "title": "Online Learning and Stochastic Approximations",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "author": [
      {
        "given": "Leon",
        "family": "Bottou"
      }
    ],
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: online-learning"
  },
  {
    "id": "kajx4KkT",
    "type": "paper-conference",
    "abstract": "There is an increasing realization that algorithmic inductive biases are central in preventing overfitting; empirically, we often see a benign overfitting phenomenon in overparameterized settings for natural learning algorithms, such as stochastic gradient descent (SGD), where little to no explicit regularization has been employed. This work considers this issue in arguably the most basic setting: constant-stepsize SGD (with iterate averaging) for linear regression in the overparameterized regime. Our main result provides a sharp excess risk bound, stated in terms of the full eigenspectrum of the data covariance matrix, that reveals a bias-variance decomposition characterizing when generalization is possible: (i) the variance bound is characterized in terms of an effective dimension and (ii) the bias bound provides a sharp geometric characterization in terms of the location of the initial iterate (and how it aligns with the data covariance matrix). We reflect on a number of notable differences between the algorithmic regularization afforded by (unregularized) SGD in comparison to ordinary least squares (minimum-norm interpolation) and ridge regression.",
    "container-title": "Proceedings of Thirty Fourth Conference on Learning Theory",
    "event-title": "Conference on Learning Theory",
    "language": "en",
    "page": "4633-4635",
    "publisher": "PMLR",
    "source": "proceedings.mlr.press",
    "title": "Benign Overfitting of Constant-Stepsize SGD for Linear Regression",
    "URL": "https://proceedings.mlr.press/v134/zou21a.html",
    "author": [
      {
        "family": "Zou",
        "given": "Difan"
      },
      {
        "family": "Wu",
        "given": "Jingfeng"
      },
      {
        "family": "Braverman",
        "given": "Vladimir"
      },
      {
        "family": "Gu",
        "given": "Quanquan"
      },
      {
        "family": "Kakade",
        "given": "Sham"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          5,
          1
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          7,
          21
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:http://proceedings.mlr.press/v134/zou21a.html"
  },
  {
    "id": "43wsMmMv",
    "type": "article-journal",
    "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.",
    "container-title": "Journal of Machine Learning Research",
    "ISSN": "1533-7928",
    "issue": "85",
    "page": "2825-2830",
    "source": "jmlr.org",
    "title": "Scikit-learn: Machine Learning in Python",
    "title-short": "Scikit-learn",
    "URL": "http://jmlr.org/papers/v12/pedregosa11a.html",
    "volume": "12",
    "author": [
      {
        "family": "Pedregosa",
        "given": "Fabian"
      },
      {
        "family": "Varoquaux",
        "given": "Gaël"
      },
      {
        "family": "Gramfort",
        "given": "Alexandre"
      },
      {
        "family": "Michel",
        "given": "Vincent"
      },
      {
        "family": "Thirion",
        "given": "Bertrand"
      },
      {
        "family": "Grisel",
        "given": "Olivier"
      },
      {
        "family": "Blondel",
        "given": "Mathieu"
      },
      {
        "family": "Prettenhofer",
        "given": "Peter"
      },
      {
        "family": "Weiss",
        "given": "Ron"
      },
      {
        "family": "Dubourg",
        "given": "Vincent"
      },
      {
        "family": "Vanderplas",
        "given": "Jake"
      },
      {
        "family": "Passos",
        "given": "Alexandre"
      },
      {
        "family": "Cournapeau",
        "given": "David"
      },
      {
        "family": "Brucher",
        "given": "Matthieu"
      },
      {
        "family": "Perrot",
        "given": "Matthieu"
      },
      {
        "family": "Duchesnay",
        "given": "Édouard"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          5,
          1
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2011"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://jmlr.org/papers/v12/pedregosa11a.html"
  },
  {
    "id": "Mm8xIDqq",
    "type": "article-journal",
    "abstract": "LIBLINEAR is an open source library for large-scale linear\nclassification.  It supports logistic regression and linear support\nvector machines.  We provide easy-to-use command-line tools and\nlibrary calls for users and developers.  \nComprehensive documents are available for both beginners and advanced\nusers.  Experiments demonstrate that LIBLINEAR is very efficient on\nlarge sparse data sets.",
    "container-title": "Journal of Machine Learning Research",
    "ISSN": "1533-7928",
    "issue": "61",
    "page": "1871-1874",
    "source": "www.jmlr.org",
    "title": "LIBLINEAR: A Library for Large Linear Classification",
    "title-short": "LIBLINEAR",
    "URL": "http://jmlr.org/papers/v9/fan08a.html",
    "volume": "9",
    "author": [
      {
        "family": "Fan",
        "given": "Rong-En"
      },
      {
        "family": "Chang",
        "given": "Kai-Wei"
      },
      {
        "family": "Hsieh",
        "given": "Cho-Jui"
      },
      {
        "family": "Wang",
        "given": "Xiang-Rui"
      },
      {
        "family": "Lin",
        "given": "Chih-Jen"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          5,
          1
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2008"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.jmlr.org/papers/v9/fan08a.html"
  }
]
