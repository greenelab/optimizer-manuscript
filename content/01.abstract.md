## Abstract {.page_break_before}

### Motivation

Most models can be fit to data using various optimization approaches.
While model choice is frequently reported in machine-learning-based research, optimizers are not often noted.
We applied two different implementations of LASSO logistic regression implemented in Python's scikit-learn package, using two different optimization approaches (coordinate descent and stochastic gradient descent), to predict driver mutation presence or absence from gene expression across 84 pan-cancer driver genes.
Across varying levels of regularization, we compared performance and model sparsity between optimizers.

### Results

In general, we found that coordinate descent (implemented in the `liblinear` library) tended to outperform SGD for the best-performing level of regularization.
For most driver genes, the best-performing `liblinear` model was more highly regularized than the best-performing SGD model.
However, SGD models generally resisted overfitting as regularization strength decreased and model complexity increased.
Our SGD results provide a counterexample to the conventional wisdom in classification for gene expression data, suggesting that explicit regularization is not always necessary to address high dimensionality and redundancy in the data.

### Availability and implementation

The code used to carry out the analyses in this study is available at <https://github.com/greenelab/pancancer-evaluation/tree/master/01_stratified_classification>. Performance/regularization strength curves for all genes in the Vogelstein et al. 2013 dataset are available at <https://doi.org/10.6084/m9.figshare.22728644>.
