## Abstract {.page_break_before}

### Motivation

Most models can be fit to data using various optimization approaches.
While model choice is frequently reported in machine-learning-based research, optimizers are not often noted.
We applied two different implementations of LASSO logistic regression implemented in Python's scikit-learn package, using two different optimization approaches (coordinate descent and stochastic gradient descent), to predict driver mutation presence or absence from gene expression across 84 pan-cancer driver genes.
Across varying levels of regularization, we compared performance and model sparsity between optimizers.

### Results

In general, we found that coordinate descent (implemented in the `liblinear` library) tended to outperform SGD for the best-performing level of regularization.
For most driver genes, the best-performing `liblinear` model was more highly regularized than the best-performing SGD model.
Moreover, SGD models generally resisted overfitting as regularization strength decreased and model complexity increased.
While the `liblinear` results for this problem match the conventional wisdom cautioning against overfitting, the SGD results contradict it.
We believe that the choice of optimizers should be clearly reported as a part of the model selection and validation process, to allow readers and reviewers to better understand the context in which results have been generated.

### Availability and implementation

The code used to carry out the analyses in this study is available at <https://github.com/greenelab/pancancer-evaluation/tree/master/01_stratified_classification>. Performance/regularization strength curves for all genes in the Vogelstein et al. 2013 dataset are available at <https://doi.org/10.6084/m9.figshare.22728644>.
