## Abstract {.page_break_before}

### Motivation

Most models can be fit to data using various optimization approaches.
While model choice is frequently reported in machine-learning-based research, optimizers are not often noted.
We applied two different implementations of LASSO logistic regression implemented in Python's scikit-learn package, using two different optimization approaches (coordinate descent and stochastic gradient descent), to predict driver mutation presence or absence from gene expression across 84 pan-cancer driver genes.
Across varying levels of regularization, we compared performance and model sparsity between optimizers.

### Results

In general, we found that coordinate descent (implemented in the `liblinear` library) and SGD tended to perform comparably after model selection and tuning.
However, SGD models generally resisted overfitting as regularization strength decreased and model complexity increased, and `liblinear` models tended to be less robust to overfitting.
We also found that learning rate tuning on a held-out dataset was critical for SGD to achieve competitive performance with `liblinear`.
We believe that the choice of optimizers should be clearly reported as a part of the model selection and validation process, to allow readers and reviewers to better understand the context in which results have been generated.

### Availability and implementation

The code used to carry out the analyses in this study is available at <https://github.com/greenelab/pancancer-evaluation/tree/master/01_stratified_classification>. Performance/regularization strength curves for all genes in the Vogelstein et al. 2013 dataset are available at <https://doi.org/10.6084/m9.figshare.22728644>.
